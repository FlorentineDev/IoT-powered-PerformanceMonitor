{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f1ec3e-008c-472e-b837-df38dedf75f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "class Bronze():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/xxxx\"\n",
    "        self.BOOTSTRAP_SERVER = \"xxxx\"\n",
    "        self.JAAS_MODULE = \"xxxx\"\n",
    "        self.CLUSTER_API_KEY = \"xxxx\"\n",
    "        self.CLUSTER_API_SECRET = \"xxxx\"\n",
    "        self.cast_schema = ArrayType(StructType([\n",
    "                StructField(r'(PDH-CSV 4.0) (ora solare Europa occidentale)(-60)', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Memoria\\MByte disponibili', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Interfaccia di rete(Realtek USB GbE Family Controller)\\Byte ricevuti/sec', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Interfaccia di rete(Intel[R] Dual Band Wireless-AC 7265)\\Byte ricevuti/sec', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Interfaccia di rete(Realtek USB GbE Family Controller)\\Byte inviati/sec', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Interfaccia di rete(Intel[R] Dual Band Wireless-AC 7265)\\Byte inviati/sec', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Disco fisico(_Total)\\Media byte letti da disco', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Disco fisico(_Total)\\Media byte scritti su disco', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Processore(_Total)\\% Tempo privilegiato', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Processore(_Total)\\% Tempo processore', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Processore(_Total)\\% Tempo utente', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Ritardo input utente per sessione(0)\\Ritardo massimo input', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Ritardo input utente per sessione(1)\\Ritardo massimo input', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Ritardo input utente per sessione(Max)\\Ritardo massimo input', StringType(), True),\n",
    "                StructField(r'\\\\SUPERPORTATILE\\Ritardo input utente per sessione(Average)\\Ritardo massimo input', StringType(), True),\n",
    "                StructField(r'Genera un rapporto con informazioni dettagliate sullo stato delle risorse hardware locali, sui tempi di risposta del sistema e sui processi eseguiti nel computer locale. Tali informazioni consentono di identificare le possibili cause dei problemi di prestazioni. Per eseguire questo Insieme agenti di raccolta dati, � necessario essere almeno membri del gruppo Administrators locale o di un gruppo equivalente.', StringType(), True),\n",
    "        ]))\n",
    "\n",
    "    def ingestFromKafka(self, startingTime = 1):\n",
    "        return ( spark.readStream\n",
    "                        .format(\"kafka\")            \n",
    "                        .option(\"kafka.bootstrap.servers\", self.BOOTSTRAP_SERVER)\n",
    "                        .option(\"kafka.security.protocol\", \"xxxx\")\n",
    "                        .option(\"kafka.sasl.mechanism\", \"xxxx\")\n",
    "                        .option(\"kafka.sasl.jaas.config\", f\"{self.JAAS_MODULE} required username='{self.CLUSTER_API_KEY}' password='{self.CLUSTER_API_SECRET}';\")\n",
    "                        .option(\"subscribe\", \"xxxx\")\n",
    "                        .option(\"maxOffsetsPerTrigger\", 10)\n",
    "                        .option(\"startingTimestamp\", startingTime)\n",
    "                        .option(\"startingOffsetsByTimestampStrategy\", \"latest\")\n",
    "                        .load()\n",
    "                )\n",
    "       \n",
    "    def process(self, startingTime = 1):\n",
    "        print(f\"Starting Bronze Stream...\", end='')\n",
    "        rawDF = self.ingestFromKafka(startingTime)\n",
    "        invoicesDF =rawDF.select(rawDF.key.cast(\"string\").alias(\"kafka_key\"),\n",
    "                                rawDF.value.cast(\"string\").alias(\"value\"),\n",
    "                                rawDF.topic.cast(\"string\").alias(\"kafka_topic\"),\n",
    "                                rawDF.timestamp.alias(\"kafka_timestamp\"))\n",
    "        df_explode_1 =invoicesDF.withColumn(\"json\",F.explode(F.from_json(\"value\",self.cast_schema)))\n",
    "        dfexpanded = df_explode_1.select(\"*\", \"json.*\") \n",
    "        dfexpanded_2 = dfexpanded\n",
    "        for field_name in dfexpanded.schema.names:\n",
    "            if field_name.startswith(\"\\\\\\\\SUPERPORTATILE\"):\n",
    "                dfexpanded_2 = dfexpanded_2.withColumn(field_name, dfexpanded_2[field_name].cast(FloatType()))\n",
    "        dfexpanded_3 = dfexpanded_2.withColumn('native_timestamp',to_timestamp(col(\"`(PDH-CSV 4.0) (ora solare Europa occidentale)(-60)`\"),\"MM/dd/yyyy HH:mm:ss.SSS\"))\n",
    "        df_clean = dfexpanded_3.drop('json','value','kafka_topic','(PDH-CSV 4.0) (ora solare Europa occidentale)(-60)','Genera un rapporto con informazioni dettagliate sullo stato delle risorse hardware locali, sui tempi di risposta del sistema e sui processi eseguiti nel computer locale. Tali informazioni consentono di identificare le possibili cause dei problemi di prestazioni. Per eseguire questo Insieme agenti di raccolta dati, � necessario essere almeno membri del gruppo Administrators locale o di un gruppo equivalente.')\n",
    "        df_c = df_clean.select([col(c).alias(\n",
    "                c.replace( '(', '')\n",
    "                .replace( ')', '')\n",
    "                .replace( ',', '')\n",
    "                .replace( ';', '')\n",
    "                .replace( '{', '')\n",
    "                .replace( '}', '')\n",
    "                .replace( '\\n', '')\n",
    "                .replace( '\\t', '')\n",
    "                .replace( ' ', '_')\n",
    "            ) for c in df_clean.columns])\n",
    "        df_c2 = df_c.filter(df_c.native_timestamp.isNotNull())\n",
    "        df_c3 = df_c2.dropDuplicates(['native_timestamp'])\n",
    "        sQuery =  ( df_c3.writeStream\n",
    "                                .queryName(\"bronze-ingestion\")\n",
    "                                .option(\"checkpointLocation\", f\"{base_data_dir}/chekpoint/cpl_perfmoningestion\")\n",
    "                                .outputMode(\"append\")\n",
    "                                .toTable(\"perfmoningestion\")           \n",
    "                        )\n",
    "        print(\"Done\")\n",
    "        return sQuery\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "performance-monitor-ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
